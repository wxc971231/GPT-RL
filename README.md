# CleanGPT

**Read in other languages: [English](README.md), [ä¸­æ–‡](README_zh.md).**

CleanGPT is a clean, educational, and extensible PyTorch-based training framework for GPT-style language models. Built upon [NanoGPT](https://github.com/karpathy/nanoGPT), this project aims to provide researchers and practitioners with a well-structured, easy-to-understand codebase for training transformer-based language models.

## âœ¨ Features

### ğŸš€ Training Infrastructure
- **Distributed Training**: Multi-GPU training support with PyTorch DDP
- **Automatic Mixed Precision**: Memory-efficient training with `torch.cuda.amp`
- **Model Compilation**: Accelerated training with `torch.compile` (PyTorch 2.0+)
- **Resume Training**: Seamless training resumption from snapshots
- **Early Stopping**: Automatic overfitting prevention

### ğŸ“Š Advanced Scheduling
- **Dynamic Learning Rate**: Cosine/linear decay with warmup
- **Weight Decay Scheduling**: Adaptive regularization
- **Gradient Accumulation**: Dynamic batch size scaling
- **Macro-Batch Training**: Efficient large-scale dataset handling

### ğŸ”§ Model Support
- **NanoGPT**: Lightweight GPT implementation
- **Llama Architecture**: Modern transformer with RMSNorm and SwiGLU
- **Flexible Configuration**: Easy model architecture customization

### ğŸ“ˆ Monitoring & Management
- **Wandb Integration**: Real-time training metrics visualization
- **Checkpoint Management**: Automatic best model preservation
- **Memory-Efficient Data Loading**: `np.memmap` for large datasets
- **Comprehensive Evaluation**: Built-in scoring for mathematical tasks

## ğŸ—‚ï¸ Project Structure

```
CleanGPT/
â”œâ”€â”€ configs/                # Experiment configurations
â”‚   â”œâ”€â”€ base.py                # Base configuration class
â”‚   â”œâ”€â”€ shakespeare.py         # Shakespeare dataset config
â”‚   â”œâ”€â”€ mathAdder.py           # Addition task config
â”‚   â”œâ”€â”€ mathMulitplier.py      # Multiplication task config
â”‚   â””â”€â”€ tinystory.py           # TinyStory dataset config
â”œâ”€â”€ data/                   # Dataset preparation and loading
â”‚   â”œâ”€â”€ data.py                # Core data loading utilities
â”‚   â”œâ”€â”€ shakespeare_char/      # Character-level Shakespeare
â”‚   â”œâ”€â”€ adder/                 # Mathematical addition dataset
â”‚   â”œâ”€â”€ multiplier/            # Mathematical multiplication dataset
â”‚   â””â”€â”€ tinystory/             # TinyStory dataset
â”œâ”€â”€ model/                  # Model architectures
â”‚   â”œâ”€â”€ NanoGPT.py             # GPT implementation
â”‚   â””â”€â”€ llama.py               # Llama architecture
â”œâ”€â”€ train/                  # Training infrastructure
â”‚   â”œâ”€â”€ trainer.py             # Main training logic
â”‚   â”œâ”€â”€ train_ddp.py           # Distributed training entry
â”‚   â”œâ”€â”€ scheduler.py           # Learning rate and parameter scheduling
â”‚   â””â”€â”€ config.py              # Command-line argument parsing
â”œâ”€â”€ eval/                   # Evaluation and testing
â”‚   â”œâ”€â”€ evaluater.py           # Model evaluation framework
â”‚   â”œâ”€â”€ eval_ddp.py            # Distributed evaluation
â”‚   â””â”€â”€ script_score.py        # Task-specific scoring
â””â”€â”€ utils/                  # Utility functions
    â”œâ”€â”€ utils.py               # General utilities
    â””â”€â”€ utils_model.py         # Model-specific utilities
```

## ğŸš€ Quick Start

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/wxc971231/CleanGPT.git
   cd CleanGPT
   ```

2. **Install PyTorch**
   
   Install PyTorch according to your CUDA version from the [official website](https://pytorch.org/get-started/previous-versions/). PyTorch 2.0+ is recommended for model compilation features.

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Training Examples

#### 1. Shakespeare Character-Level Model

```bash
# Prepare dataset
cd data/shakespeare_char
python prepare.py
cd ../..

# Train model
torchrun --standalone --nproc_per_node=1 train/train_ddp.py
```

#### 2. Mathematical Addition Task

```bash
# Prepare dataset
cd data/adder
python prepare.py
cd ../..

# Train model (modify experiment_name in train_ddp.py to 'Adder_NanoGPT')
torchrun --standalone --nproc_per_node=1 train/train_ddp.py
```

#### 3. Multi-GPU Training

```bash
# Train on 4 GPUs
torchrun --standalone --nproc_per_node=4 train/train_ddp.py
```

## ğŸ“‹ Supported Datasets

| Dataset | Description | Task Type | Vocab Size |
|---------|-------------|-----------|------------|
| **Shakespeare** | Character-level text from Shakespeare works | Language Modeling | ~65 |
| **TinyStory** | Children's stories dataset | Language Modeling | ~32K |
| **Mathematical Addition** | N-digit addition problems | calculate | 10-13 |
| **Mathematical Multiplication** | N-digit multiplication | calculate | 10-13 |

## âš™ï¸ Configuration System

CleanGPT uses a flexible configuration system with experiment-specific settings:

```python
# Example: Shakespeare configuration
class ShakespeareNanoGPTConfig(BaseExperimentConfig):
    def get_args_ready(self):
        args = self.get_base_args()
        
        # Model settings
        args.model = 'NanoGPT'
        args.n_position = 256
        args.n_layer = 4
        args.n_head = 8
        args.n_embed = 256
        
        # Training settings
        args.lr_max = 1e-3
        args.batch_size_per_gpu = 32
        args.train_iters = 5000
        
        return args
```

## ğŸ“Š Monitoring and Evaluation

### Wandb Integration

Training metrics are automatically logged to Wandb:
- Training/Validation loss
- Learning rate schedules
- Model parameters
- Dataset coverage

### Mathematical Task Evaluation

For addition and multiplication tasks, the framework includes specialized evaluation:

```python
# Automatic accuracy calculation
accuracy = eval_score_adder(model, dataset, tokenizer)
print(f"Addition accuracy: {accuracy:.2%}")
```

## ğŸ”§ Advanced Features

### Dynamic Parameter Scheduling

```python
# Learning rate with warmup and cosine decay
args.lr_warmup_ratio = 0.05
args.lr_decay_ratio = 0.95
args.lr_decay_style = "cosine"

# Dynamic gradient accumulation
args.ga_begin = 4
args.ga_end = 16
args.ga_incr_style = "linear"
```

### Memory-Efficient Data Loading

```python
# Large datasets loaded with memory mapping
data = np.load(data_path, mmap_mode='r')
# No need to load entire dataset into memory
```

### Model Compilation (PyTorch 2.0+)

```python
# Automatic model compilation for speedup
if args.compile:
    model = torch.compile(model)
```

## ğŸ“ˆ Performance Tips

1. **Use Mixed Precision**: Enable `--use-amp` for memory efficiency
2. **Optimize Batch Size**: Use gradient accumulation for effective large batches
3. **Enable Compilation**: Use `--compile` with PyTorch 2.0+
4. **Multi-GPU Training**: Scale to multiple GPUs with DDP
5. **Memory Mapping**: Efficient for datasets larger than RAM